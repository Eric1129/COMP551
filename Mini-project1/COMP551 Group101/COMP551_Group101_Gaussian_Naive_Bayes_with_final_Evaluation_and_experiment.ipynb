{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP551 Group101 Gaussian Naive Bayes with final Evaluation and experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA_G9KgPBUB2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# COMP551 Group101 Naive Bayes, Evaluation, and Experiment\n",
        " Eric Shen 260798146\n",
        " \n",
        " Modified for use with Hybrid Naive Bayes by Edwin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjyX38zbe_99",
        "colab_type": "text"
      },
      "source": [
        "## Useful Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqxGWL_re_Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weac0WMtAyuc",
        "colab_type": "text"
      },
      "source": [
        "## Gaussian Naive Bayes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ygtPUyN7Bq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#============================================================================================================================================================================================================================================================================\n",
        "#\n",
        "# GAUSSIAN NAIVE-BAYES MODEL CLASS\n",
        "#\n",
        "#   @Author: Edwin Pan of Group 101 of Winter 2020 COMP551 at McGill University\n",
        "#\n",
        "#============================================================================================================================================================================================================================================================================\n",
        "\n",
        "class GaussianNaiveBayesModel:\n",
        "\n",
        "  #Constructor and Instance Variables\n",
        "  #@takes no_of_features, the amount of features that will be taken in\n",
        "  #@takes no_of_classes, which is how many outputs are possible and being read in in integers (ie 2 indicates boolean, 3 indicates ternary, etc.)\n",
        "  def __init__(self,no_of_features,no_of_classes):\n",
        "    if(no_of_classes==1):\n",
        "      print(\"Will not create model with one possible output.\")\n",
        "      return None\n",
        "    self.class_prior = np.zeros( no_of_classes )\n",
        "    self.mean = np.zeros( (no_of_features, no_of_classes) )\n",
        "    self.variance = np.zeros( (no_of_features, no_of_classes) )\n",
        "    return\n",
        "\n",
        "  #helper binarizeGBClasses function\n",
        "  #Takes a one-dimensional matrix and replaces elements of \"g\" with 1 and elements of \"b\" with 0.\n",
        "  def __binarizeGBClasses__(self,matrix):\n",
        "    for i in range(len(matrix)):\n",
        "      if matrix[i]==\"g\":\n",
        "        matrix[i] = 1\n",
        "      elif matrix[i]==\"b\":\n",
        "        matrix[i] = 0\n",
        "    return matrix\n",
        "\n",
        "  #helper normal function\n",
        "  #Gives the probability of x occuring in normal function given the mean and variance\n",
        "  def __normal__(self,x,m,v):\n",
        "    if(v==0):\n",
        "      return 0\n",
        "    pi = 3.1415926535898535\n",
        "    base = 1\n",
        "    power = 1\n",
        "    numerator = 1\n",
        "    denumerator = (2*pi)**0.5*v\n",
        "    e_numerator = 0-(x-m)**2\n",
        "    e_denumerator = v**2*2\n",
        "    base = numerator/denumerator\n",
        "    power = e_numerator/e_denumerator\n",
        "    return base*np.exp(power)\n",
        "\n",
        "  #Fit function\n",
        "  #@Takes DataFrame of training data, Double of learningRate, and Integer of gradientDescentIteration\n",
        "  #@Returns nothing\n",
        "  #Fits the model\n",
        "  def fit(self, trainingDataFeatures, trainingDataClasses):\n",
        "    trainingDataClasses = self.__binarizeGBClasses__(trainingDataClasses)\n",
        "    lists_by_class = []\n",
        "    #Calculate Class Prior first\n",
        "    for c in range( len(self.class_prior) ):\n",
        "      lists_by_class.append( [] )\n",
        "      for i in range( trainingDataClasses.size ):\n",
        "        if( trainingDataClasses[i] == c ):\n",
        "          self.class_prior[c] = self.class_prior[c] + 1\n",
        "          lists_by_class[c].append(i)\n",
        "      self.class_prior[c] /= len(trainingDataClasses)\n",
        "    #Now calculate the means and variances for all feature and class pairs\n",
        "    for c in range( len(self.class_prior) ):\n",
        "      #We first first calculate the Mean for each feature's classes:\n",
        "      #Save the sum of the features for each class in each mean[f,c].\n",
        "      for i in lists_by_class[c]:\n",
        "        for f in range( len(self.mean) ):\n",
        "          self.mean[f][c] += trainingDataFeatures[i][f]*1\n",
        "      #Now divide each sum of features by the amount of features they were to obtain the mean.\n",
        "      for f in range( len(self.mean) ):\n",
        "        self.mean[f][c] /= ( len(lists_by_class[c]) )\n",
        "      #Now that we know the mean for this class of value, we can now calculate the variances:\n",
        "      #Save the sum the squared errors per instance of feature per class in variance[f][c].\n",
        "      for i in lists_by_class[c]:\n",
        "        for f in range( len(self.variance) ):\n",
        "          self.variance[f][c] += (trainingDataFeatures[i][f]*1 - self.mean[f][c])**2\n",
        "      #Now that we have the sum, we now take the average error squared and then squareroot that average\n",
        "      for f in range( len(self.variance) ):\n",
        "        self.variance[f][c] = ( self.variance[f][c]/(len(lists_by_class[c])) )**0.5\n",
        "    return #Thus we now have our class priors as well as the means and variances values for calculating probabilities.\n",
        "\n",
        "  #Predict function\n",
        "  #@Takes in an input Series of input datapoint\n",
        "  #@Returns the classification of the input datapoint classifications\n",
        "  def predict(self, x):\n",
        "    #Get an array in which we will put the probabilities of each class into\n",
        "    output_probabilities = np.zeros( len(self.class_prior) )\n",
        "    #Calculate the probability of each classification type\n",
        "    for c in range( len(output_probabilities) ):\n",
        "      prior_class_probability = 0\n",
        "      conditional_probability = 1\n",
        "      marginal_probability = 0\n",
        "      #Prior\n",
        "      prior_class_probability = self.class_prior[c]\n",
        "      #Conditional of X given the c currently observed\n",
        "      conditional_probability = 1\n",
        "      for f in range( len(self.mean) ):\n",
        "        if( self.variance[f][c] == 0):\n",
        "          continue\n",
        "        conditional_probability *= self.__normal__(x[f], self.mean[f][c], self.variance[f][c])\n",
        "      #Marginal of X over all classes\n",
        "      for k in range( len(output_probabilities) ):\n",
        "        marginal_partial = 1;\n",
        "        for f in range( len(self.mean) ):\n",
        "          if( self.variance[f][k] == 0):\n",
        "            continue\n",
        "          marginal_partial *= self.__normal__(x[f], self.mean[f][k], self.variance[f][k])\n",
        "        marginal_probability += marginal_partial\n",
        "      #Calculate the output probability of this class c\n",
        "      output_probabilities[c] = prior_class_probability*conditional_probability/marginal_probability\n",
        "    greater_class = 0;\n",
        "    for c in range(1,len(output_probabilities)):\n",
        "      if( output_probabilities[greater_class] < output_probabilities[c] ):\n",
        "        greater_class = c\n",
        "    return greater_class\n",
        "\n",
        "  #evaluate_acc method.\n",
        "  #@Takes in Training Data (NxD Matrix) and Classifications (Nx1 Matrix);\n",
        "  #@Returns the accuracy of the current weights\n",
        "  def evaluate_acc(self, X, Y):\n",
        "    everythingRight = 0\n",
        "    everything = 0\n",
        "    for i in range( len(X) ):  #For each instance\n",
        "      if( self.predict( X[i] ) == Y[i] ):  #Use the model, predict the output with obtained features, and tally the result.\n",
        "        everythingRight += 1\n",
        "      everything += 1                 #don't forget the total count.\n",
        "    return everythingRight/everything"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orsB_8a4Ayjw",
        "colab_type": "text"
      },
      "source": [
        "## Normaliztion from Edwin's code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5cMXhzK3Dee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#==============================================================================================================\n",
        "#\n",
        "#   Vector Normalizer\n",
        "#\n",
        "#     Takes an input vector of numbers and normalizes its values between 0 and 1.\n",
        "#\n",
        "#==============================================================================================================\n",
        "def normalize_vector(vector):\n",
        "  #Obtain Normalization Values\n",
        "  min_value = vector[0]\n",
        "  max_value = vector[0]\n",
        "  for i in range(len(vector)):\n",
        "    if vector[i] < min_value:\n",
        "      min_value = vector[i]\n",
        "    elif vector[i] > max_value:\n",
        "      max_value = vector[i]\n",
        "  #Normalize all vector elements\n",
        "  for i in range(len(vector)):\n",
        "    vector[i] = ( vector[i] - min_value )/(max_value-min_value)\n",
        "  return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi7nIS2EBnwX",
        "colab_type": "text"
      },
      "source": [
        "## Processes for four datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycc0WMG3R3PT",
        "colab_type": "text"
      },
      "source": [
        "### train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cRltSxcE2HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(mydataset: np.ndarray, k: int, Normalize: bool):\n",
        "\n",
        "    #Normalize all feature input columns\n",
        "    if(Normalize):\n",
        "      for column in range( len(mydataset[0]) - 1 ):\n",
        "        vector = []\n",
        "        for instance in range(len(mydataset)):\n",
        "          vector.append( mydataset[instance][column] )\n",
        "        vector = normalize_vector(vector)\n",
        "        for instance in range(len(mydataset)):\n",
        "          mydataset[instance][column] = vector[instance]\n",
        "\n",
        "    np.random.shuffle(mydataset)\n",
        "    rows = mydataset.shape[0]\n",
        "    mydataset_train = mydataset[: (int)(k * rows/10), :]\n",
        "    mydataset_test = mydataset[(int)(k * rows/10):, :]\n",
        "    \n",
        "    mydataset_train_x = mydataset_train[:, :-1]\n",
        "    mydataset_train_y = mydataset_train[:, -1]\n",
        "    mydataset_test_x = mydataset_test[:, :-1]\n",
        "    mydataset_test_y = mydataset_test[:, -1]\n",
        "\n",
        "    return mydataset_train_x, mydataset_train_y, mydataset_test_x, mydataset_test_y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hHJgDxqD_t2",
        "colab_type": "text"
      },
      "source": [
        "### Process Ionosphere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LXkEy5Qhsm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_ionosphere():\n",
        "\n",
        "    ionosphere = pd.read_csv(\"ionosphere.csv\", header=None)\n",
        "    ionosphere          = ionosphere.drop([1],axis=1).to_numpy()\n",
        "\n",
        "    classIndex = len(ionosphere[0]) -1\n",
        "    for i in range(len(ionosphere)): \n",
        "      if( ionosphere[i][classIndex] == \"g\" ): \n",
        "          ionosphere[i][classIndex] = 1\n",
        "      else:\n",
        "          ionosphere[i][classIndex] = 0\n",
        "\n",
        "\n",
        "    ionosphere = np.array(ionosphere[0:])\n",
        "\n",
        "    return train_test_split(ionosphere, 9, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIoVdIjGB6zN",
        "colab_type": "text"
      },
      "source": [
        "### One Hot Encoding for adult data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtZI0uRQsFYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "def clean(data):\n",
        "    data = data.dropna(axis='index')\n",
        "    return data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def preprocess(data):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(data['workclass'])\n",
        "    data['workclass'] = le.transform(data['workclass'])\n",
        "    le.fit(data['education'])\n",
        "    data['education'] = le.transform(data['education'])\n",
        "    le.fit(data['marital-status'])\n",
        "    data['marital-status'] = le.transform(data['marital-status'])\n",
        "    le.fit(data['occupation'])\n",
        "    data['occupation'] = le.transform(data['occupation'])\n",
        "    le.fit(data['relationship'])\n",
        "    data['relationship'] = le.transform(data['relationship'])\n",
        "    le.fit(data['race'])\n",
        "    data['race'] = le.transform(data['race'])\n",
        "    le.fit(data['sex'])\n",
        "    data['sex'] = le.transform(data['sex'])\n",
        "    le.fit(data['native-country'])\n",
        "    data['native-country'] = le.transform(data['native-country'])\n",
        "    le.fit(data['id'])\n",
        "    data['id'] = le.transform(data['id'])\n",
        "    temp = data.drop(columns='id').copy()\n",
        "    data = data[temp.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]\n",
        "    return data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def one_hot_encoder(data):\n",
        "    # values = np.array(data)\n",
        "    values = data.to_numpy()\n",
        "    # integer encode\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded = label_encoder.fit_transform(values)\n",
        "    # binary encode\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "    return onehot_encoded\n",
        "\n",
        "\n",
        "def one_hot_encoder_without_sklean(data):\n",
        "    temp = data.drop(columns='id')\n",
        "    id = data.id\n",
        "    data = pd.get_dummies(temp, prefix_sep='_', drop_first=True)\n",
        "    data['id'] = id\n",
        "    data.head()\n",
        "    data = data.replace(['<=50K', '>50K'], [0, 1])\n",
        "    temp = data.drop(columns='id').copy()\n",
        "    data = data[temp.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]\n",
        "    return data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# show the distribution of the positive vs. negative classes\n",
        "def show_povsneg(data):\n",
        "    sumid = [(data.id == 0).sum(), (data.id == 1).sum()]\n",
        "    xl = ['<=50', '>50']\n",
        "    plt.bar(x=xl, height=sumid)\n",
        "    for i, v in zip(xl, sumid):\n",
        "        plt.annotate(str(v), xy=(i, v), color='black', va='center', size=11)\n",
        "    plt.ylabel('amount')\n",
        "    plt.xlabel('wage')\n",
        "    plt.title('distribution of different wages(adult)')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaJMrqOZD1El",
        "colab_type": "text"
      },
      "source": [
        "### Process Adult"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE1hwddD135R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_Adult():\n",
        "  data = pd.read_csv('adult.data', engine='python', sep=',\\s', na_values=['?'],\n",
        "                    names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
        "                           'marital-status', 'occupation','relationship', 'race', 'sex', \n",
        "                           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'id'])\n",
        "  target = data['id']\n",
        "\n",
        "  data = clean(data)\n",
        "  data = preprocess(data)\n",
        "\n",
        "  workclass = one_hot_encoder(data['workclass'])\n",
        "  # print(workclass.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(workclass)], axis=1)\n",
        "  education = one_hot_encoder(data['education'])\n",
        "  # print(education.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(education)], axis=1)\n",
        "  marital_status = one_hot_encoder(data['marital-status'])\n",
        "  # print(marital_status.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(marital_status)], axis=1)\n",
        "  occupation = one_hot_encoder(data['occupation'])\n",
        "  # print(occupation.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(occupation)], axis=1)\n",
        "  relationship = one_hot_encoder(data['relationship'])\n",
        "  # print(relationship.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(relationship)], axis=1)\n",
        "  race = one_hot_encoder(data['race'])\n",
        "  # print(race.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(race)], axis=1)\n",
        "  native_country = one_hot_encoder(data['native-country'])\n",
        "  # print(native_country.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(native_country)], axis=1)\n",
        "\n",
        "  # data=data.replace(['Male','Female'],[1,0])\n",
        "  # data=data.replace(['<=50K','>50K'],[0,1])\n",
        "\n",
        "  del data['workclass']\n",
        "  del data['education']\n",
        "  del data['marital-status']\n",
        "  del data['occupation']\n",
        "  del data['race']\n",
        "  del data['relationship']\n",
        "  del data['native-country']\n",
        "  ids = data['id'].copy()\n",
        "  del data[\"id\"]\n",
        "  data.insert(data.shape[1], \"id\", ids)\n",
        "\n",
        "  adult_dataset_result = data.to_numpy()\n",
        "\n",
        "  return train_test_split(adult_dataset_result, 9, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrTXe9fNEVZ3",
        "colab_type": "text"
      },
      "source": [
        "### Process wines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znUi6m7Z_KI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_wines():\n",
        "    with open(\"winequality-white.csv\", 'r') as f:\n",
        "        wines = list(csv.reader(f, delimiter=\";\"))\n",
        "    global wines_header\n",
        "    wines_header = np.array(wines[0])  # with label header\n",
        "    wines = np.array(wines[1:], dtype=np.float)  # with label\n",
        "\n",
        "    # clean malinformed values by deleting the rows they inhabit\n",
        "    invalid_index = []\n",
        "    for i in range(len(wines)):\n",
        "        for number in wines[i]:\n",
        "            if math.isnan(number):\n",
        "                np.delete(wines, i, 0)\n",
        "\n",
        "    # differentiate labels\n",
        "    for i in tqdm(range(len(wines[:, -1]))):\n",
        "        if wines[:, -1][i] > 5:\n",
        "            wines[:, -1][i] = 1\n",
        "        else:\n",
        "            wines[:, -1][i] = 0\n",
        "\n",
        "    return train_test_split(wines, 9, True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zweg_4jMroS",
        "colab_type": "text"
      },
      "source": [
        "### Process Breast Cancer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr79YJ7IAibJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_tumors():\n",
        "    with open(\"breast-cancer-wisconsin.csv\", 'r') as f:\n",
        "        tumors = list(csv.reader(f, delimiter=\";\"))\n",
        "\n",
        "    global tumors_header\n",
        "    tumors_header = [\"clump thickness\", \"cell size\", \"cell shape\", \"marginal adhesion\", \\\n",
        "                     \"single epithelial cell size\", \"number of bare nuclei\", \"bland chromatin\", \\\n",
        "                     \"number of normal nuclei\", \"mitosis\", \"label\"]  # with label header but no IDs\n",
        "\n",
        "    # highlight malinformed values\n",
        "    invalid_index = []\n",
        "    for i in tqdm(range(len(tumors))):\n",
        "        tumors[i] = tumors[i][0].split(\",\")\n",
        "        for j in range(len(tumors[i])):\n",
        "            if tumors[i][j].isnumeric() == False:\n",
        "                invalid_index.append(i)  # the whole row\n",
        "        # differentiate labels\n",
        "        if int(tumors[i][-1]) <= 2:\n",
        "            tumors[i][-1] = '0'\n",
        "        else:\n",
        "            tumors[i][-1] = '1'\n",
        "\n",
        "    # clean malinformed values by deleting the rows they inhabit\n",
        "    invalid_index.sort(reverse=True)\n",
        "    for i in invalid_index:\n",
        "        tumors.remove(tumors[i])\n",
        "\n",
        "    tumors = np.array(tumors[0:], dtype=np.float)\n",
        "    tumors = tumors[:, 1:]\n",
        "\n",
        "    return train_test_split(tumors, 9, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZctIbJ0oIJUh",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Um8SDt_5SE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation(prediction: np.ndarray, groundtruth: np.ndarray):\n",
        "    # sanity check\n",
        "    if len(prediction) != len(groundtruth):\n",
        "        raise TypeError\n",
        "    \n",
        "    tn,fp,fn,tp = 0,0,0,0 #true negative, false positive, false negative, true positive\n",
        "    \n",
        "    for i in range(len(prediction)):\n",
        "        if prediction[i] == 0 and groundtruth[i] == 0:\n",
        "            tn += 1\n",
        "        if prediction[i] == 1 and groundtruth[i] == 0:\n",
        "            fp += 1\n",
        "        if prediction[i] == 0 and groundtruth[i] == 1:\n",
        "            fn += 1\n",
        "        if prediction[i] == 1 and groundtruth[i] == 1:\n",
        "            tp += 1\n",
        "    return tn,fp,fn,tp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBchU2P__rNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_chunks(data_split,indices):\n",
        "    indices = list(indices).sort()\n",
        "    if len([indices]) < 2:\n",
        "        return data_split[0]\n",
        "    data_merged = data_split[indices[0]]\n",
        "    indices.remove(indices[0]) #remove the first element so that it does not get re-merged\n",
        "    for i in indices:\n",
        "        data_merged = np.concatenate(data_merged,data_split[i],axis=0)\n",
        "        \n",
        "    return data_merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnfVTjMmDVBn",
        "colab_type": "text"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgyvHBU4-uls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "#============================================================================================================================================================================================================================================================================\n",
        "#\n",
        "# K-CROSS VALIDATION\n",
        "#\n",
        "#   @Author: Edwin Pan of Group 101 of Winter 2020 COMP551 at McGill University\n",
        "#\n",
        "#   This section is a script for applying K-Cross Validation \n",
        "#\n",
        "#============================================================================================================================================================================================================================================================================\n",
        "\n",
        "def kCrossValidate(k,cleanModel,featuresDataset,classificationsDataset,learningRate=None,gradientDescentIterations=None):\n",
        "\n",
        "  #Create a list ready to keep track of all the models ever made.\n",
        "  allKModels = []\n",
        "\n",
        "  #First, create all of the dataset partitions needed. \n",
        "  features_test_block =             []\n",
        "  classifications_test_block  =     []\n",
        "  features_training_block =         []\n",
        "  classifications_training_block =  []\n",
        "  dataset_size = classificationsDataset.size  #Total number of instances\n",
        "  for fold in range(k):\n",
        "    #Get the subsets\n",
        "    lower_index = dataset_size*fold//k        #Get the lower bound index from the full dataset for the test dataset\n",
        "    upper_index = dataset_size*(fold+1)//k    #Get the upper bound idnex from the full dataset for the test dataset\n",
        "    #The Test Block of size 1/k per k\n",
        "    features_test_block.append(         featuresDataset[(lower_index):(upper_index)]                )\n",
        "    classifications_test_block.append(      classificationsDataset[(lower_index):(upper_index)]                 )\n",
        "    #The Training Block of size (k-1)k per k\n",
        "    if(lower_index==0):   #If we're dealing with the first fold\n",
        "      features_training_block.append(                featuresDataset[upper_index:] )\n",
        "      classifications_training_block.append(  classificationsDataset[upper_index:]   ) \n",
        "    elif(upper_index == dataset_size):    #If we're dealing with the last fold\n",
        "      features_training_block.append(                featuresDataset[:lower_index] )  \n",
        "      classifications_training_block.append(  classificationsDataset[:lower_index]  )\n",
        "    else:\n",
        "      #Note that these two following operations only work if lower_index is not 0 and upper_index is not the upper bound such that we get empty matrices.\n",
        "      new_block_size = dataset_size - upper_index + lower_index;\n",
        "      no_of_features = len(featuresDataset[0])\n",
        "      #Before making the block, we need to make sure our new array will contain the right datatype. If we find string, make our array hold objects.\n",
        "      datatype = type(float)\n",
        "      for f in range(no_of_features):\n",
        "        if not isinstance( featuresDataset[0][f], float ) and not isinstance( featuresDataset[0][f], int):\n",
        "          datatype = object\n",
        "          break\n",
        "      #Make the new block\n",
        "      new_features_block = np.zeros( (new_block_size,no_of_features), dtype=datatype )\n",
        "      new_classes_block = np.zeros( new_block_size )\n",
        "      j = 0 #Row being checked\n",
        "      for i in range( lower_index ):\n",
        "        for f in range(no_of_features):\n",
        "          new_features_block[j][f] = featuresDataset[i][f]\n",
        "        new_classes_block[j] = classificationsDataset[i]\n",
        "        j += 1\n",
        "      for i in range( upper_index, dataset_size ):\n",
        "        for f in range(no_of_features):\n",
        "          new_features_block[j][f] = featuresDataset[i][f]\n",
        "        new_classes_block[j] = classificationsDataset[i]\n",
        "        j += 1\n",
        "      features_training_block.append( new_features_block )\n",
        "      classifications_training_block.append( new_classes_block )\n",
        "      #features_training_block.append(         np.concatenate(featuresDataset[:lower_index],(featuresDataset[upper_index:])) )   \n",
        "      #classifications_training_block.append(  np.concatenate(classificationsDataset[:lower_index],(classificationsDataset[upper_index:]))   ) \n",
        "      \n",
        "  #Now we start training and testing the model.\n",
        "  average_accuracy = 0;\n",
        "  for fold in range(k):\n",
        "    model = copy.deepcopy(cleanModel)\n",
        "    if(learningRate==None or gradientDescentIterations==None):\n",
        "      model.fit(trainingDataFeatures= features_training_block[fold], \n",
        "                trainingDataClasses=  classifications_training_block[fold]\n",
        "                )\n",
        "    else:\n",
        "      model.fit(trainingDataFeatures=     features_training_block[fold], \n",
        "                trainingDataClasses=      classifications_training_block[fold], \n",
        "                learningRate=             learningRate, \n",
        "                gradientDescentIterations=gradientDescentIterations,\n",
        "                beVerbose= False\n",
        "                )\n",
        "    fold_accuracy = model.evaluate_acc( X=(features_test_block[fold]), Y=(classifications_test_block[fold]) )\n",
        "    average_accuracy += fold_accuracy\n",
        "    #print(\"Accuracy of fold number \" + str(fold+1) + \" is \" + str(fold_accuracy) + \".\")\n",
        "    allKModels.append(model)\n",
        "    continue\n",
        "  average_accuracy /= k\n",
        "  #Return the average accuracy obtained\n",
        "  return average_accuracy, allKModels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVyEaMzXzt-4",
        "colab_type": "text"
      },
      "source": [
        "## Model Averager\n",
        "\n",
        "Given enough time, we would like to combine all of the k models obtained through the k-fold cross validation to create a new model for use on predicting the classifications on the test dataset. Combining of the models would come down to essentially averaging each weight among the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptcz-tLhz0u0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "def averageHNBModels(models):\n",
        "  numberOfModels = len(models)\n",
        "  newModel = copy.deepcopy(model[0])\n",
        "  for k, v in newModel.feature_models.values():\n",
        "    if newModel.feature_models[k].__type__ == \"BINARY\":\n",
        "      average_featuremodel_mean = 0\n",
        "      average_featuremodel_variance = 0\n",
        "      for m in models:\n",
        "        m.feature_models[k].mean\n",
        "    elif newModel.feature_models[k].__type__ == \"CATEGORICAL\":\n",
        "      pass\n",
        "    elif newModel.feature_models[k].__type__ == \"GAUSSIAN\":\n",
        "      pass\n",
        "      #INCOMPLETE CODE\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7xe_cDnDY22",
        "colab_type": "text"
      },
      "source": [
        "## Four Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otg7t5fdlA89",
        "colab_type": "code",
        "outputId": "683eaede-357b-4460-81d3-4a49be781204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import time\n",
        "ionosphere_train_x, ionosphere_train_y, ionosphere_test_x, ionosphere_test_y = process_ionosphere()\n",
        "start = time.time()\n",
        "ionosphere_HNBModel = GaussianNaiveBayesModel( len(ionosphere_train_x[0]), 2 )\n",
        "average_accuracy, allModels = kCrossValidate(5,ionosphere_HNBModel,ionosphere_train_x,ionosphere_train_y)\n",
        "ionosphere_HNBModel.fit(ionosphere_train_x,ionosphere_train_y)\n",
        "final_test_accuracy = ionosphere_HNBModel.evaluate_acc(ionosphere_test_x,ionosphere_test_y)\n",
        "end = time.time()\n",
        "print(\"Gaussian Naive Bayes K-Cross Validated (K=5) accuracy of our model is \" + str(average_accuracy) +\".\")\n",
        "print(\"Final accuracy on Ionosphere's testing set: \", final_test_accuracy)\n",
        "print(\"Gaussian Naive Bayes on Ionosphere train set time: \", (end - start)/5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is 0.8222222222222222.\n",
            "Final accuracy on Ionosphere's testing set:  0.8333333333333334\n",
            "Hybrid Naive Bayes on Ionosphere train set time:  0.11877923011779785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bby3b1UR_3iY",
        "colab_type": "code",
        "outputId": "662677ec-7e57-43e9-8f36-721b5459397f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import time\n",
        "cancer_train_x, cancer_train_y, cancer_test_x, cancer_test_y = process_tumors()\n",
        "start = time.time()\n",
        "cancer_HNBModel = GaussianNaiveBayesModel( len(cancer_train_x[0]), 2 )\n",
        "average_accuracy, allModels = kCrossValidate(5,cancer_HNBModel,cancer_train_x,cancer_train_y)\n",
        "cancer_HNBModel.fit(cancer_train_x,cancer_train_y)\n",
        "final_test_accuracy = cancer_HNBModel.evaluate_acc(cancer_test_x,cancer_test_y)\n",
        "end = time.time()\n",
        "print(\"Gaussian Naive Bayes K-Cross Validated (K=5) accuracy of our model is \" + str(average_accuracy) +\".\")\n",
        "print(\"Final accuracy on Cancer's testing set: \", final_test_accuracy)\n",
        "print(\"Gaussian Naive Bayes on Cancer train set time: \", (end - start)/5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 699/699 [00:00<00:00, 170543.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is 0.964121018259363.\n",
            "Final accuracy on Cancer's testing set:  0.9420289855072463\n",
            "Hybrid Naive Bayes on Cancer train set time:  0.07529220581054688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLQmSwFwsplA",
        "colab_type": "code",
        "outputId": "08e82918-ffa7-4cbf-c9db-decd39b3f998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import time\n",
        "Adult_train_x, Adult_train_y, Adult_test_x, Adult_test_y  = process_Adult()\n",
        "start = time.time()\n",
        "adult_HNBModel = GaussianNaiveBayesModel( len(Adult_train_x[0]), 2 )\n",
        "average_accuracy, allModels = kCrossValidate(5,adult_HNBModel,Adult_train_x,Adult_train_y)\n",
        "adult_HNBModel.fit(Adult_train_x,Adult_train_y)\n",
        "final_test_accuracy = adult_HNBModel.evaluate_acc(Adult_test_x,Adult_test_y)\n",
        "end = time.time()\n",
        "print(\"Gaussian Naive Bayes K-Cross Validated (K=5) accuracy of our model is \" + str(average_accuracy) +\".\")\n",
        "print(\"Final accuracy on Adult's testing set: \", final_test_accuracy)\n",
        "print(\"Gaussian Naive Bayes on Adult train set time: \", (end - start)/5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:114: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is 0.7952909439268457.\n",
            "Final accuracy on Adult's testing set:  0.7895962732919255\n",
            "Hybrid Naive Bayes on Adult train set time:  20.904246473312377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhglqTPR-0Ho",
        "colab_type": "code",
        "outputId": "c713c376-0dd2-4edb-80ae-474ac29903bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import time\n",
        "x_wines_train, y_wines_train, x_wines_test, y_wines_test = process_wines()\n",
        "start = time.time()\n",
        "wine_HNBModel = GaussianNaiveBayesModel( len(x_wines_train[0]), 2 )\n",
        "average_accuracy, allModels = kCrossValidate(5,wine_HNBModel,x_wines_train,y_wines_train)\n",
        "wine_HNBModel.fit(x_wines_train,y_wines_train)\n",
        "final_test_accuracy = wine_HNBModel.evaluate_acc(x_wines_test,y_wines_test)\n",
        "end = time.time()\n",
        "print(\"Gaussian Naive Bayes K-Cross Validated (K=5) accuracy of our model is \" + str(average_accuracy) +\".\")\n",
        "print(\"Final accuracy on Wines testing set: \", final_test_accuracy)\n",
        "print(\"Gaussian Naive Bayes on Wines train set time: \", (end - start)/5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4898/4898 [00:00<00:00, 495721.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is 0.7019080049727041.\n",
            "Final accuracy on Wines testing set:  0.7448979591836735\n",
            "Hybrid Naive Bayes on Wines train set time:  0.6234371185302734\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}