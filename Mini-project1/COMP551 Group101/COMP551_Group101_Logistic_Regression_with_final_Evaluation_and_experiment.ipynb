{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP551 Group101 Logistic Regression with final Evaluation and experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA_G9KgPBUB2",
        "colab_type": "text"
      },
      "source": [
        "# COMP551 Group101 Logistic Regression, Evaluation, and Experiment\n",
        " Eric Shen 260798146\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weac0WMtAyuc",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ygtPUyN7Bq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Logistic(object):\n",
        "    def __init__(self, learningR, Iterations):\n",
        "        self.learningRate = learningR\n",
        "        self.gradientDescentIterations = Iterations\n",
        "        self.weights = []\n",
        "\n",
        "    def sigmoid(self, gamma):\n",
        "        if gamma < 0:\n",
        "            return 1 - 1 / (1 + math.exp(gamma))\n",
        "        else:\n",
        "            return 1 / (1 + math.exp(-gamma))\n",
        "\n",
        "    def addup(self, MatrixW, MatrixX, Y):\n",
        "        addOn = 0.0\n",
        "        for num in range(0, len(MatrixX)):\n",
        "            addOn = addOn + MatrixW[num] * MatrixX[num]\n",
        "        addOn = Y - self.sigmoid(addOn)\n",
        "        MatrixAdd = []\n",
        "\n",
        "        for num in range(0, len(MatrixX)):\n",
        "            MatrixAdd.append(MatrixX[num] * addOn)\n",
        "\n",
        "        return MatrixAdd\n",
        "\n",
        "    def fit(self, trainingDataMatrixX, trainingDataMatrixY):\n",
        "        numOfRow = len(trainingDataMatrixX)\n",
        "        numOfColumn = len(trainingDataMatrixX[0])\n",
        "        MatrixAddAll = []\n",
        "        if len(self.weights) > 0:\n",
        "            for num in range(0, numOfColumn + 1):\n",
        "                MatrixAddAll.append(0)\n",
        "        else:\n",
        "            for num in range(0, numOfColumn + 1):\n",
        "                self.weights.append(0.01)\n",
        "                MatrixAddAll.append(0)\n",
        "\n",
        "        for num in range(0, self.gradientDescentIterations):\n",
        "            for numOne in range(0, numOfRow):\n",
        "                MaX = trainingDataMatrixX[numOne]\n",
        "                MaX = np.append([1],MaX)\n",
        "                matrixAdd = self.addup(self.weights, MaX, trainingDataMatrixY[numOne])\n",
        "                for numTwo in range(0, numOfColumn + 1):\n",
        "                    MatrixAddAll[numTwo] = matrixAdd[numTwo] + MatrixAddAll[numTwo]\n",
        "            for numThree in range(0, numOfColumn + 1):\n",
        "                MatrixAddAll[numThree] = self.learningRate * MatrixAddAll[numThree]\n",
        "            for numFour in range(0, numOfColumn + 1):\n",
        "                self.weights[numFour] = self.weights[numFour] + MatrixAddAll[numFour]\n",
        "            for numFive in range(0, numOfColumn + 1):\n",
        "                MatrixAddAll[numFive] = 0\n",
        "        return\n",
        "\n",
        "    def predict(self, trainingDataMatrixX):\n",
        "        outPutY = []\n",
        "        for numOne in range(0, len(trainingDataMatrixX)):\n",
        "            MatrixTemp = trainingDataMatrixX[numOne]\n",
        "            sig = self.weights[0]\n",
        "            for num in range(0, len(MatrixTemp)):\n",
        "                sig = sig + self.weights[num + 1] * MatrixTemp[num]\n",
        "            if self.sigmoid(sig) >= 0.5:\n",
        "                outPutY.append(1)\n",
        "            else:\n",
        "                outPutY.append(0)\n",
        "        return outPutY\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orsB_8a4Ayjw",
        "colab_type": "text"
      },
      "source": [
        "## Normaliztion from Edwin's code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5cMXhzK3Dee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#==============================================================================================================\n",
        "#\n",
        "#   Vector Normalizer\n",
        "#\n",
        "#     Takes an input vector of numbers and normalizes its values between 0 and 1.\n",
        "#\n",
        "#==============================================================================================================\n",
        "def normalize_vector(vector):\n",
        "  #Obtain Normalization Values\n",
        "  min_value = vector[0]\n",
        "  max_value = vector[0]\n",
        "  for i in range(len(vector)):\n",
        "    if vector[i] < min_value:\n",
        "      min_value = vector[i]\n",
        "    elif vector[i] > max_value:\n",
        "      max_value = vector[i]\n",
        "  #Normalize all vector elements\n",
        "  for i in range(len(vector)):\n",
        "    vector[i] = ( vector[i] - min_value )/(max_value-min_value)\n",
        "  return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi7nIS2EBnwX",
        "colab_type": "text"
      },
      "source": [
        "## Processes for four datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycc0WMG3R3PT",
        "colab_type": "text"
      },
      "source": [
        "### train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cRltSxcE2HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(mydataset: np.ndarray, k: int, Normalize: bool):\n",
        "\n",
        "    #Normalize all feature input columns\n",
        "    if(Normalize):\n",
        "      for column in range( len(mydataset[0]) - 1 ):\n",
        "        vector = []\n",
        "        for instance in range(len(mydataset)):\n",
        "          vector.append( mydataset[instance][column] )\n",
        "        vector = normalize_vector(vector)\n",
        "        for instance in range(len(mydataset)):\n",
        "          mydataset[instance][column] = vector[instance]\n",
        "\n",
        "    # First I did a shuffle for whole set\n",
        "    np.random.shuffle(mydataset)\n",
        "    rows = mydataset.shape[0]\n",
        "    # I add k/10 of set to train set, the rest is test set\n",
        "    mydataset_train = mydataset[: (int)(k * rows/10), :]\n",
        "    mydataset_test = mydataset[(int)(k * rows/10):, :]\n",
        "    # Then I split both train set and test set into features and classification\n",
        "    mydataset_train_x = mydataset_train[:, :-1]\n",
        "    mydataset_train_y = mydataset_train[:, -1]\n",
        "    mydataset_test_x = mydataset_test[:, :-1]\n",
        "    mydataset_test_y = mydataset_test[:, -1]\n",
        "\n",
        "    return mydataset_train_x, mydataset_train_y, mydataset_test_x, mydataset_test_y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hHJgDxqD_t2",
        "colab_type": "text"
      },
      "source": [
        "### Process Ionosphere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LXkEy5Qhsm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_ionosphere():\n",
        "\n",
        "    ionosphere = pd.read_csv(\"ionosphere.csv\", header=None)\n",
        "    ionosphere          = ionosphere.drop([1],axis=1).to_numpy()\n",
        "\n",
        "    classIndex = len(ionosphere[0]) -1\n",
        "    for i in range(len(ionosphere)): \n",
        "      if( ionosphere[i][classIndex] == \"g\" ): \n",
        "          ionosphere[i][classIndex] = 1\n",
        "      else:\n",
        "          ionosphere[i][classIndex] = 0\n",
        "\n",
        "\n",
        "    ionosphere = np.array(ionosphere[0:])\n",
        "\n",
        "    return train_test_split(ionosphere, 9, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIoVdIjGB6zN",
        "colab_type": "text"
      },
      "source": [
        "### One Hot Encoding for adult data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtZI0uRQsFYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def clean(data):\n",
        "    data = data.dropna(axis='index')\n",
        "    return data.reset_index(drop=True)\n",
        "\n",
        "def preprocess(data):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(data['workclass'])\n",
        "    data['workclass'] = le.transform(data['workclass'])\n",
        "    le.fit(data['education'])\n",
        "    data['education'] = le.transform(data['education'])\n",
        "    le.fit(data['marital-status'])\n",
        "    data['marital-status'] = le.transform(data['marital-status'])\n",
        "    le.fit(data['occupation'])\n",
        "    data['occupation'] = le.transform(data['occupation'])\n",
        "    le.fit(data['relationship'])\n",
        "    data['relationship'] = le.transform(data['relationship'])\n",
        "    le.fit(data['race'])\n",
        "    data['race'] = le.transform(data['race'])\n",
        "    le.fit(data['sex'])\n",
        "    data['sex'] = le.transform(data['sex'])\n",
        "    le.fit(data['native-country'])\n",
        "    data['native-country'] = le.transform(data['native-country'])\n",
        "    le.fit(data['id'])\n",
        "    data['id'] = le.transform(data['id'])\n",
        "    temp = data.drop(columns='id').copy()\n",
        "    data = data[temp.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]\n",
        "    return data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def one_hot_encoder(data):\n",
        "    values = data.to_numpy()\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded = label_encoder.fit_transform(values)\n",
        "\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "    return onehot_encoded\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaJMrqOZD1El",
        "colab_type": "text"
      },
      "source": [
        "### Process Adult"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE1hwddD135R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_Adult():\n",
        "  data = pd.read_csv('adult.data', engine='python', sep=',\\s', na_values=['?'],\n",
        "                    names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
        "                           'marital-status', 'occupation','relationship', 'race', 'sex', \n",
        "                           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'id'])\n",
        "  target = data['id']\n",
        "\n",
        "  data = clean(data)\n",
        "  data = preprocess(data)\n",
        "\n",
        "  workclass = one_hot_encoder(data['workclass'])\n",
        "  data = pd.concat([data, pd.DataFrame(workclass)], axis=1)\n",
        "\n",
        "  education = one_hot_encoder(data['education'])\n",
        "  data = pd.concat([data, pd.DataFrame(education)], axis=1)\n",
        "\n",
        "  marital_status = one_hot_encoder(data['marital-status'])\n",
        "  data = pd.concat([data, pd.DataFrame(marital_status)], axis=1)\n",
        "\n",
        "  occupation = one_hot_encoder(data['occupation'])\n",
        "  data = pd.concat([data, pd.DataFrame(occupation)], axis=1)\n",
        "\n",
        "  relationship = one_hot_encoder(data['relationship'])\n",
        "  data = pd.concat([data, pd.DataFrame(relationship)], axis=1)\n",
        "\n",
        "  race = one_hot_encoder(data['race'])\n",
        "  data = pd.concat([data, pd.DataFrame(race)], axis=1)\n",
        "\n",
        "  native_country = one_hot_encoder(data['native-country'])\n",
        "  data = pd.concat([data, pd.DataFrame(native_country)], axis=1)\n",
        "\n",
        "  del data['workclass']\n",
        "  del data['education']\n",
        "  del data['marital-status']\n",
        "  del data['occupation']\n",
        "  del data['race']\n",
        "  del data['relationship']\n",
        "  del data['native-country']\n",
        "  ids = data['id'].copy()\n",
        "  del data[\"id\"]\n",
        "  data.insert(data.shape[1], \"id\", ids)\n",
        "\n",
        "  adult_dataset_result = data.to_numpy()\n",
        "\n",
        "  return train_test_split(adult_dataset_result, 9, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrTXe9fNEVZ3",
        "colab_type": "text"
      },
      "source": [
        "### Process wines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znUi6m7Z_KI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_wines():\n",
        "    with open(\"winequality-white.csv\", 'r') as f:\n",
        "        wines = list(csv.reader(f, delimiter=\";\"))\n",
        "    global wines_header\n",
        "    wines_header = np.array(wines[0])  # with label header\n",
        "    wines = np.array(wines[1:], dtype=np.float)  # with label\n",
        "\n",
        "    # clean malinformed values by deleting the rows they inhabit\n",
        "    invalid_index = []\n",
        "    for i in range(len(wines)):\n",
        "        for number in wines[i]:\n",
        "            if math.isnan(number):\n",
        "                np.delete(wines, i, 0)\n",
        "\n",
        "    # differentiate labels\n",
        "    for i in tqdm(range(len(wines[:, -1]))):\n",
        "        if wines[:, -1][i] > 5:\n",
        "            wines[:, -1][i] = 1\n",
        "        else:\n",
        "            wines[:, -1][i] = 0\n",
        "\n",
        "    return train_test_split(wines, 9, True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zweg_4jMroS",
        "colab_type": "text"
      },
      "source": [
        "### Process Breast Cancer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr79YJ7IAibJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_cancer():\n",
        "    with open(\"breast-cancer-wisconsin.csv\", 'r') as f:\n",
        "        tumors = list(csv.reader(f, delimiter=\";\"))\n",
        "\n",
        "    global tumors_header\n",
        "    tumors_header = [\"clump thickness\", \"cell size\", \"cell shape\", \"marginal adhesion\", \\\n",
        "                     \"single epithelial cell size\", \"number of bare nuclei\", \"bland chromatin\", \\\n",
        "                     \"number of normal nuclei\", \"mitosis\", \"label\"]  # with label header but no IDs\n",
        "    invalid_index = []\n",
        "    for i in tqdm(range(len(tumors))):\n",
        "        tumors[i] = tumors[i][0].split(\",\")\n",
        "        for j in range(len(tumors[i])):\n",
        "            if tumors[i][j].isnumeric() == False:\n",
        "                invalid_index.append(i) \n",
        "        # change labels into binary\n",
        "        if int(tumors[i][-1]) <= 2:\n",
        "            tumors[i][-1] = '0'\n",
        "        else:\n",
        "            tumors[i][-1] = '1'\n",
        "\n",
        "    # I clean malformed values by deleting the whole row\n",
        "    invalid_index.sort(reverse=True)\n",
        "    for i in invalid_index:\n",
        "        tumors.remove(tumors[i])\n",
        "\n",
        "    tumors = np.array(tumors[0:], dtype=np.float)\n",
        "    tumors = tumors[:, 1:]\n",
        "\n",
        "    return train_test_split(tumors, 9, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZctIbJ0oIJUh",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Um8SDt_5SE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation(prediction: np.ndarray, groundtruth: np.ndarray):\n",
        "    # sanity check\n",
        "    if len(prediction) != len(groundtruth):\n",
        "        raise TypeError\n",
        "    \n",
        "    tn,fp,fn,tp = 0,0,0,0 # these stands for true negative, false positive, false negative, true positive\n",
        "    \n",
        "    for i in range(len(prediction)):\n",
        "        if prediction[i] == 0 and groundtruth[i] == 0:\n",
        "            tn += 1\n",
        "        if prediction[i] == 1 and groundtruth[i] == 0:\n",
        "            fp += 1\n",
        "        if prediction[i] == 0 and groundtruth[i] == 1:\n",
        "            fn += 1\n",
        "        if prediction[i] == 1 and groundtruth[i] == 1:\n",
        "            tp += 1\n",
        "    return tn,fp,fn,tp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBchU2P__rNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_chunks(data_split,indices):\n",
        "    indices = list(indices).sort()\n",
        "    if len([indices]) < 2:\n",
        "        return data_split[0]\n",
        "    data_merged = data_split[indices[0]]\n",
        "    indices.remove(indices[0])\n",
        "    for i in indices:\n",
        "        data_merged = np.concatenate(data_merged,data_split[i],axis=0)\n",
        "        \n",
        "    return data_merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ12EursDaBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion_m(prediction: np.ndarray, groundtruth: np.ndarray):\n",
        "    tn,fp,fn,tp = evaluation(prediction,groundtruth)\n",
        "    confusion_matrix = [[tp, fp],[fn,tn]]\n",
        "    return confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvEIFZNw_ygl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_acc(prediction: np.ndarray, groundtruth: np.ndarray):\n",
        "    tn,fp,fn,tp = evaluation(prediction,groundtruth)\n",
        "    return 1.0*(tp+tn)/(tp+tn+fp+fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnfVTjMmDVBn",
        "colab_type": "text"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgyvHBU4-uls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation(model,x: np.ndarray,y: np.ndarray, k: int):\n",
        "    \n",
        "    data = np.zeros((len(x),len(x[0])+1))\n",
        "    #combine and save to \"data\"\n",
        "    for i in range(len(x)):\n",
        "        data[i] = np.append(x[i],[y[i]])\n",
        "    # I shuffle the whole dataset then just split    \n",
        "    np.random.shuffle(data)\n",
        "    data_split = np.array_split(data,k)\n",
        "    indices = set(range(k))\n",
        "    # the list contains all the output accuracies by k folds\n",
        "    acc_list = [] \n",
        "    for fold in range(k):\n",
        "        # merge the numpy arrays except for the validation set for training\n",
        "        other_indices = indices - set([fold])\n",
        "        training_set = merge_chunks(data_split,other_indices)\n",
        "        test_set = data_split[fold]\n",
        "        x_train = training_set[:,:-1]\n",
        "        y_train = training_set[:,-1]\n",
        "        x_test = test_set[:,:-1]\n",
        "        y_test = test_set[:,-1]\n",
        "        \n",
        "        model.fit(x_train,y_train)\n",
        "        y_prediction = model.predict(x_test)\n",
        "        \n",
        "        acc_list.append(evaluate_acc(y_prediction,y_test))\n",
        "    return sum(acc_list) / len(acc_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7xe_cDnDY22",
        "colab_type": "text"
      },
      "source": [
        "## Four Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otg7t5fdlA89",
        "colab_type": "code",
        "outputId": "7117a309-43b3-4826-8165-529b411dcf0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import time\n",
        "ionosphere_train_x, ionosphere_train_y, ionosphere_test_x, ionosphere_test_y = process_ionosphere()\n",
        "start = time.time()\n",
        "clf = Logistic(0.001,1000)\n",
        "print(\"LR on Ionosphere's training set by using 5-fold CV: \",cross_validation(clf,ionosphere_train_x,ionosphere_train_y,5))\n",
        "end = time.time()\n",
        "print(\"Final accuracy on Ionosphere's testing set: \", evaluate_acc(clf.predict(ionosphere_test_x),ionosphere_test_y))\n",
        "print(\"LR on Ionosphere train set time: \", (end - start)/5)"
      ],
      "execution_count": 553,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR on Ionosphere's training set by using 5-fold CV:  0.8603174603174603\n",
            "Final accuracy on Ionosphere's testing set:  0.8611111111111112\n",
            "LR on Ionosphere train set time:  2.4480565547943116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLQmSwFwsplA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cc41d5e3-4a71-43fd-c616-ec08bd93260c"
      },
      "source": [
        "import time\n",
        "Adult_train_x, Adult_train_y, Adult_test_x, Adult_test_y  = process_Adult()\n",
        "start = time.time()\n",
        "clf = Logistic(0.001,1000)\n",
        "print(\"LR on Adult\",cross_validation(clf,Adult_train_x,Adult_train_y,5))\n",
        "end = time.time()\n",
        "print(\"Final accuracy on Adult's testing set: \", evaluate_acc(clf.predict(Adult_test_x),Adult_test_y))\n",
        "print(\"LR on Adult train set time: \", (end - start)/5)"
      ],
      "execution_count": 564,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR on Adult 0.819357996220147\n",
            "Final accuracy on Adult's testing set:  0.8260869565217391\n",
            "LR on Adult train set time:  363.43937520980836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bby3b1UR_3iY",
        "colab_type": "code",
        "outputId": "ae071e0b-b163-4882-87d4-7144921025d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import time\n",
        "cancer_train_x, cancer_train_y, cancer_test_x, cancer_test_y = process_cancer()\n",
        "start = time.time()\n",
        "clf = Logistic(0.001,1000)\n",
        "print(\"LR on Cancer's training set by using 5-fold CV: \",cross_validation(clf,cancer_train_x, cancer_train_y,5))\n",
        "end = time.time()\n",
        "print(\"Final accuracy on Cancer's testing set: \", evaluate_acc(clf.predict(cancer_test_x),cancer_test_y))\n",
        "print(\"LR on Cancer train set time: \", (end - start)/5)"
      ],
      "execution_count": 559,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 699/699 [00:00<00:00, 138653.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR on Cancer's training set by using 5-fold CV:  0.9608556577369052\n",
            "Final accuracy on Cancer's testing set:  1.0\n",
            "LR on Cancer train set time:  2.233937931060791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhglqTPR-0Ho",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "de6fd018-a2d4-4d40-d96f-a1b22d27467f"
      },
      "source": [
        "import time\n",
        "x_wines_train, y_wines_train, x_wines_test, y_wines_test = process_wines()\n",
        "start = time.time()\n",
        "clf = Logistic(0.001,1000)\n",
        "print(\"LR on wines' training set by using 5-fold CV: \",cross_validation(clf,x_wines_train,y_wines_train,5))\n",
        "end = time.time()\n",
        "print(\"Final accuracy on Wine's testing set: \", evaluate_acc(clf.predict(x_wines_test),y_wines_test))\n",
        "print(\"LR on wines train set time: \", (end - start)/5)"
      ],
      "execution_count": 560,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4898/4898 [00:00<00:00, 510770.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR on wines' training set by using 5-fold CV:  0.7454675551643283\n",
            "Final accuracy on Wine's testing set:  0.7163265306122449\n",
            "LR on wines train set time:  17.35471272468567\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}