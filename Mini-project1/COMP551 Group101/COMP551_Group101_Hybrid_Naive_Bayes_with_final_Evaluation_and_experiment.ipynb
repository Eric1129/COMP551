{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP551 Group101 Hybrid Naive Bayes with final Evaluation and experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA_G9KgPBUB2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# COMP551 Group101 Naive Bayes, Evaluation, and Experiment\n",
        " Eric Shen 260798146\n",
        " \n",
        " Modified for use with Hybrid Naive Bayes by Edwin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjyX38zbe_99",
        "colab_type": "text"
      },
      "source": [
        "## Useful Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqxGWL_re_Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weac0WMtAyuc",
        "colab_type": "text"
      },
      "source": [
        "## Hybrid Naive Bayes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ygtPUyN7Bq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#============================================================================================================================================================================================================================================================================\n",
        "#\n",
        "# HYBRID NAIVE-BAYES MODEL CLASS\n",
        "#\n",
        "#   @Author: Edwin Pan of Group 101 of Winter 2020 COMP551 at McGill University\n",
        "#\n",
        "#============================================================================================================================================================================================================================================================================\n",
        "\n",
        "class HybridNaiveBayesModel:\n",
        "\n",
        "  #The strategy for figuring out whether or not each feature scanned in is categorical (binary or multinomial) or gaussian will be to save\n",
        "  #parametres for predicting conditional and joint probabilities for each feature as an object which will have a descriptor for how\n",
        "  #it is meant to be used. The object, namely its class, are defined as follows.\n",
        "  #NOTE: THE ASSUMPTION WITH HAVING MULTIPLE CLASSIFICATIONS IS THAT EACH CLASSIFICATION IS MERELY ENUMERATED STARTED AT 0 TO C-1.\n",
        "  class FeatureModel:\n",
        "    __UNDEFINED_TYPE__ = \"UNFIT\"\n",
        "    __BINARY_TYPE__ = \"BINARY\"\n",
        "    __CATEGORICAL_TYPE__ = \"CATEGORICAL\"\n",
        "    __GAUSSIAN_TYPE__ = \"GAUSSIAN\"\n",
        "    __UNCONSEQUENTIAL_TYPE__ = \"UNCONSEQUENTIAL\"\n",
        "\n",
        "    def __init__(self,name):\n",
        "      self.name = name\n",
        "      self.__type__ = self.__UNDEFINED_TYPE__\n",
        "      return\n",
        "\n",
        "    #Fit this feature with the data of a single feature and the associated classification data.\n",
        "    def fit(self,single_feature_set,single_feature_set_classifications):\n",
        "      #First we need to figure out what kind of type our feature is.\n",
        "      #Getting some stats for analysis of our input features\n",
        "      feature_inputs = []\n",
        "      is_gaussian_by_floats = False #The presence of floating point numbers that aren't whole numbers indicate gaussian.\n",
        "      is_categorical_by_strings = False #The presence of strings that do not represent numbers indicates a categorical nature.\n",
        "      #Loop over full set of inputs and draw some data or conclusions\n",
        "      for instance in single_feature_set:\n",
        "        #If we know it's Gaussian, we need not look further\n",
        "        if is_gaussian_by_floats:\n",
        "          break\n",
        "        #Avoid repeating values but analyze new ones\n",
        "        instance_already_contained = False\n",
        "        for already_contained in feature_inputs:\n",
        "          if already_contained == instance:\n",
        "            instance_already_contained = True\n",
        "            break\n",
        "        if not instance_already_contained:\n",
        "          if isinstance(instance,float) and instance%1 != 0:\n",
        "            is_gaussian_by_floats = True\n",
        "          elif isinstance(instance,str) and not instance.replace('.','').isdigit():\n",
        "            is_categorical_by_strings = True\n",
        "          feature_inputs.append(instance)\n",
        "      no_of_feature_inputs = len(feature_inputs)\n",
        "\n",
        "      #Now let's figure out what output classes we need and how many there are of each\n",
        "      instances_of_classification = {}\n",
        "      for c in single_feature_set_classifications:\n",
        "        if c in instances_of_classification:\n",
        "          instances_of_classification[c] = instances_of_classification[c] + 1\n",
        "        else:\n",
        "          instances_of_classification[c] = 1\n",
        "      no_of_classifications = len(instances_of_classification)\n",
        "\n",
        "      #Deciding based on stats what type our feature is\n",
        "      if is_gaussian_by_floats: #If there were non-whole number floats then it's Gaussian for sure.\n",
        "        self.__type__ = self.__GAUSSIAN_TYPE__\n",
        "      elif is_categorical_by_strings: #If there were strings that weren't numbers, then we must treat it as categorical.\n",
        "        self.__type__ = self.__CATEGORICAL_TYPE__\n",
        "      elif no_of_feature_inputs <= 1: #If this feature always produces a single input, then our data doesn't really say anything. We'll ignore it in probabilities.\n",
        "        self.__type__ = self.__UNCONSEQUENTIAL_TYPE__\n",
        "      elif no_of_feature_inputs == 2: #If there were only two input feature values\n",
        "        if 0 in instances_of_classification and 1 in instances_of_classification:   #If it's 0 and 1, it's binary\n",
        "          self.__type__ = self.__BINARY_TYPE__\n",
        "        else:  #if it's not 0's and 1's, it's binary but in a weird way that we put under the umbrella of categorical as we use a dictionary for this.\n",
        "          self.__type__ = self.__CATEGORICAL_TYPE__\n",
        "      else: #If we have been dealing integer inputs that are not binary and are in greater variety than just two inputs, it may still be gaussian.\n",
        "        feature_inputs.sort()\n",
        "        found_skipped_category = False  #WE WILL MAKE THE BAD ASUMPTION THAT IF THE FEATURE IS CATEGORICAL, WE WILL WITNESS ALL 0-TO-N CATEGORIES.\n",
        "        for i in range( len(feature_inputs) ): #Check if any numbers are skipped\n",
        "          if i != feature_inputs[i]:\n",
        "            found_skipped_category = True\n",
        "            break\n",
        "        if(found_skipped_category):\n",
        "          self.__type__ = self.__GAUSSIAN_TYPE__\n",
        "        else:\n",
        "          self.__type__ = self.__CATEGORICAL_TYPE__\n",
        "\n",
        "      #Now that we know what our inputs and outputs are, we can set up our variables.\n",
        "      if( self.__type__ == self.__GAUSSIAN_TYPE__ ):\n",
        "\n",
        "        #Knowing we are Gaussian Type, we can establish Gaussian Probability Values\n",
        "        self.mean = {}\n",
        "        self.variance = {}\n",
        "        for c in instances_of_classification:\n",
        "          self.mean[c] = 0\n",
        "          self.variance[c] = 0\n",
        "        #Calculate mean across all classifications by this feature\n",
        "        for i in range(len(single_feature_set)):\n",
        "          self.mean[ single_feature_set_classifications[i] ] += single_feature_set[i]*1\n",
        "        for c in instances_of_classification:\n",
        "          self.mean[c] = self.mean[c]/instances_of_classification[c]\n",
        "        #Calculate variance across all classifications by this feature\n",
        "        for i in range(len(single_feature_set)):\n",
        "          self.variance[ single_feature_set_classifications[i] ] += ( single_feature_set[i]*1 - self.mean[ single_feature_set_classifications[i] ] )**2\n",
        "        for c in instances_of_classification:\n",
        "          self.variance[c] = ( self.variance[c]/instances_of_classification[c] )**0.5\n",
        "        #Thus we have obtained the mean input features and variance of input features for the each classification of this feature\n",
        "\n",
        "      elif( self.__type__ == self.__BINARY_TYPE__ ):\n",
        "        #Okay well this is as straightforward as it gets. We just need the bernoulli probability constant\n",
        "        self.u = [ instances_of_classification[0]/no_of_classifications, instances_of_classification[1]/no_of_classifications ]\n",
        "        #This yields us the probabilities of obtaining u[x] for c=1. c=0 is u[(x+1)%2], ie the inverted version of u[x].\n",
        "\n",
        "      elif( self.__type__ == self.__CATEGORICAL_TYPE__ ):\n",
        "        #Here we need to calculate how much of the probability space belongs to each categorical feature.\n",
        "        self.p = {}\n",
        "        for f in feature_inputs:\n",
        "          self.p[f] = {}\n",
        "          for c in instances_of_classification:\n",
        "            self.p[f][c] = 0\n",
        "        for i in range( len(single_feature_set) ):\n",
        "          self.p[ single_feature_set[i] ][ single_feature_set_classifications[i] ] += 1\n",
        "        for f in feature_inputs:\n",
        "          for c in instances_of_classification:\n",
        "            self.p[f][c] /= len( single_feature_set )\n",
        "        #This yields us the probabilities per category of p[f] for each classification p[f][c].\n",
        "\n",
        "      #And that should complete the fitting.\n",
        "      return\n",
        "\n",
        "    #Predict - ie, produce the marginal probability of feature input x given classification c.\n",
        "    def predict(self,x,c):\n",
        "      if self.__type__ == self.__GAUSSIAN_TYPE__:\n",
        "        if(self.variance[c] == 0):\n",
        "          return 1  #Bad case: If there's no variance, ignore this feature in calculating conditionals and marginals.\n",
        "        return self.__normal__(x,self.mean[c],self.variance[c])\n",
        "      if self.__type__ == self.__BINARY_TYPE__:\n",
        "        return self.u[ (c+(int(x)+1)%2)%2 ]\n",
        "      if self.__type__ == self.__CATEGORICAL_TYPE__:\n",
        "        if x in self.p.keys():\n",
        "          return self.p[x][c]\n",
        "        else:   #If we are dealing with a category we have never seen before, we really can't say anything about it. Release a 1 such that the posterior probability basically ignores this feature.\n",
        "          return 1\n",
        "      if self.__type__ == self.__UNCONSEQUENTIAL_TYPE__:\n",
        "        return 1\n",
        "      \n",
        "\n",
        "    #helper normal function\n",
        "    #Gives the probability of x occuring in normal function given the mean and variance\n",
        "    def __normal__(self,x,m,v):\n",
        "      if(v==0):\n",
        "        return 0\n",
        "      pi = 3.1415926535898535\n",
        "      base = 1\n",
        "      power = 1\n",
        "      numerator = 1\n",
        "      denumerator = (2*pi)**0.5*v\n",
        "      e_numerator = 0-(x-m)**2\n",
        "      e_denumerator = v**2*2\n",
        "      base = numerator/denumerator\n",
        "      power = e_numerator/e_denumerator\n",
        "      return base*np.exp(power)\n",
        "\n",
        "    #Useful Debugging Method\n",
        "    def whoAreYou(self):\n",
        "      print(\"I am feature model \" + str(self.name) + \". I am a \" + self.__type__ + \" feature model.\")\n",
        "      weights_descriptors = \"\"\n",
        "      if(self.__type__ == self.__GAUSSIAN_TYPE__):\n",
        "        weights_descriptors = \"mean: \" + str( self.mean ) + \"; \\tvariance: \" + str(self.variance)\n",
        "      elif(self.__type__ == self.__BINARY_TYPE__):\n",
        "        weights_descriptors = \"u: \" + str(self.u)\n",
        "      elif(self.__type__ == self.__CATEGORICAL_TYPE__):\n",
        "        weights_descriptors = \"p: \" + str(self.p)\n",
        "      print(\"My weights follow as: \\t\" + weights_descriptors)\n",
        "      return\n",
        "\n",
        "  #End of featureModel class\n",
        "\n",
        "\n",
        "\n",
        "  #Constructor and Instance Variables\n",
        "  #@takes no_of_features, the amount of features that will be taken in\n",
        "  #@takes no_of_classifcations, which is how many outputs are possible and being read in in integers (ie 2 indicates boolean, 3 indicates ternary, etc.)\n",
        "  def __init__(self,no_of_features,no_of_classifcations):\n",
        "    if(no_of_classifcations==1):\n",
        "      print(\"Will not create model with one possible output.\")\n",
        "      return None\n",
        "    self.class_prior = np.zeros( no_of_classifcations )\n",
        "    self.feature_models = {}\n",
        "    return\n",
        "\n",
        "  #helper binarizeGBClasses function\n",
        "  #Takes a one-dimensional matrix and replaces elements of \"g\" with 1 and elements of \"b\" with 0.\n",
        "  def __binarizeGBClasses__(self,matrix):\n",
        "    for i in range(len(matrix)):\n",
        "      if matrix[i]==\"g\":\n",
        "        matrix[i] = 1\n",
        "      elif matrix[i]==\"b\":\n",
        "        matrix[i] = 0\n",
        "    return matrix\n",
        "\n",
        "  #Fit function\n",
        "  #@Takes DataFrame of training data, Double of learningRate, and Integer of gradientDescentIteration\n",
        "  #@Returns nothing\n",
        "  #Fits the model\n",
        "  def fit(self, trainingDataFeatures, trainingDataClasses):\n",
        "    #Cleaning b and g if present\n",
        "    trainingDataClasses = self.__binarizeGBClasses__(trainingDataClasses)\n",
        "    #Calculating prior\n",
        "    for c in range( len(self.class_prior) ):\n",
        "      self.class_prior[c] = 0\n",
        "      for i in range( trainingDataClasses.size ):\n",
        "        if( trainingDataClasses[i] == c ):\n",
        "          self.class_prior[c] = self.class_prior[c] + 1\n",
        "      self.class_prior[c] /= len(trainingDataClasses)\n",
        "    #Feature Fitting\n",
        "    #This class fits to datasets on a per-feature basis. As such, we iterate for each feature \n",
        "    #and create the feature_models which give us the probatilities for each feature when calculating\n",
        "    #the final posterior probability\n",
        "    for feature in range( len( trainingDataFeatures[0] ) ):\n",
        "      feature_dataset = (trainingDataFeatures[:,feature]).T\n",
        "      self.feature_models[feature] = self.FeatureModel(feature)\n",
        "      self.feature_models[feature].fit( feature_dataset, trainingDataClasses )\n",
        "    return\n",
        "\n",
        "  #Predict function\n",
        "  #@Takes in an input Series of input datapoint\n",
        "  #@Returns the classification of the input datapoint classifications\n",
        "  def predict(self, x):\n",
        "    #Get an array in which we will put the probabilities of each class into\n",
        "    output_probabilities = np.zeros( len(self.class_prior) )\n",
        "    #Calculate the probability of each classification type\n",
        "    for c in range( len(output_probabilities) ):\n",
        "      prior_class_probability = 0\n",
        "      conditional_probability = 1\n",
        "      marginal_probability = 0\n",
        "      #Prior\n",
        "      prior_class_probability = self.class_prior[c]\n",
        "      #Conditional of X given the c currently observed\n",
        "      conditional_probability = 1\n",
        "      for f in range( len(x) ):\n",
        "        conditional_probability *= self.feature_models[f].predict(x[f],c)\n",
        "      #Marginal of X over all classes\n",
        "      for k in range( len(output_probabilities) ):\n",
        "        marginal_partial = 1;\n",
        "        for f in range( len(x) ):\n",
        "          marginal_partial *= self.feature_models[f].predict(x[f],k)\n",
        "        marginal_probability += marginal_partial\n",
        "      #Calculate the output probability of this class c\n",
        "      output_probabilities[c] = prior_class_probability*conditional_probability/marginal_probability\n",
        "    greater_class = 0;\n",
        "    for c in range(1,len(output_probabilities)):\n",
        "      if( output_probabilities[greater_class] < output_probabilities[c] ):\n",
        "        greater_class = c\n",
        "    return greater_class\n",
        "\n",
        "  #evaluate_acc method.\n",
        "  #@Takes in Training Data (NxD Matrix) and Classifications (Nx1 Matrix);\n",
        "  #@Returns the accuracy of the current weights\n",
        "  def evaluate_acc(self, X, Y):\n",
        "    everythingRight = 0\n",
        "    everything = 0\n",
        "    for i in range( len(X) ):  #For each instance\n",
        "      if( self.predict( X[i] ) == Y[i] ):  #Use the model, predict the output with obtained features, and tally the result.\n",
        "        everythingRight += 1\n",
        "      everything += 1                 #don't forget the total count.\n",
        "    return everythingRight/everything"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orsB_8a4Ayjw",
        "colab_type": "text"
      },
      "source": [
        "## Normaliztion from Edwin's code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5cMXhzK3Dee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#==============================================================================================================\n",
        "#\n",
        "#   Vector Normalizer\n",
        "#\n",
        "#     Takes an input vector of numbers and normalizes its values between 0 and 1.\n",
        "#\n",
        "#==============================================================================================================\n",
        "def normalize_vector(vector):\n",
        "  #Obtain Normalization Values\n",
        "  min_value = vector[0]\n",
        "  max_value = vector[0]\n",
        "  for i in range(len(vector)):\n",
        "    if vector[i] < min_value:\n",
        "      min_value = vector[i]\n",
        "    elif vector[i] > max_value:\n",
        "      max_value = vector[i]\n",
        "  #Normalize all vector elements\n",
        "  for i in range(len(vector)):\n",
        "    vector[i] = ( vector[i] - min_value )/(max_value-min_value)\n",
        "  return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi7nIS2EBnwX",
        "colab_type": "text"
      },
      "source": [
        "## Processes for four datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycc0WMG3R3PT",
        "colab_type": "text"
      },
      "source": [
        "### train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cRltSxcE2HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(mydataset: np.ndarray, k: int, Normalize: bool):\n",
        "\n",
        "    #Normalize all feature input columns\n",
        "    if(Normalize):\n",
        "      for column in range( len(mydataset[0]) - 1 ):\n",
        "        vector = []\n",
        "        for instance in range(len(mydataset)):\n",
        "          vector.append( mydataset[instance][column] )\n",
        "        vector = normalize_vector(vector)\n",
        "        for instance in range(len(mydataset)):\n",
        "          mydataset[instance][column] = vector[instance]\n",
        "\n",
        "    np.random.shuffle(mydataset)\n",
        "    rows = mydataset.shape[0]\n",
        "    mydataset_train = mydataset[: (int)(k * rows/10), :]\n",
        "    mydataset_test = mydataset[(int)(k * rows/10):, :]\n",
        "    \n",
        "    mydataset_train_x = mydataset_train[:, :-1]\n",
        "    mydataset_train_y = mydataset_train[:, -1]\n",
        "    mydataset_test_x = mydataset_test[:, :-1]\n",
        "    mydataset_test_y = mydataset_test[:, -1]\n",
        "\n",
        "    return mydataset_train_x, mydataset_train_y, mydataset_test_x, mydataset_test_y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hHJgDxqD_t2",
        "colab_type": "text"
      },
      "source": [
        "### Process Ionosphere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LXkEy5Qhsm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_ionosphere():\n",
        "\n",
        "    ionosphere = pd.read_csv(\"ionosphere.csv\", header=None)\n",
        "    ionosphere          = ionosphere.drop([1],axis=1).to_numpy()\n",
        "\n",
        "    classIndex = len(ionosphere[0]) -1\n",
        "    for i in range(len(ionosphere)): \n",
        "      if( ionosphere[i][classIndex] == \"g\" ): \n",
        "          ionosphere[i][classIndex] = 1\n",
        "      else:\n",
        "          ionosphere[i][classIndex] = 0\n",
        "\n",
        "\n",
        "    ionosphere = np.array(ionosphere[0:])\n",
        "\n",
        "    return train_test_split(ionosphere, 9, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIoVdIjGB6zN",
        "colab_type": "text"
      },
      "source": [
        "### One Hot Encoding for adult data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtZI0uRQsFYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "def clean(data):\n",
        "    data = data.dropna(axis='index')\n",
        "    return data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def preprocess(data):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(data['workclass'])\n",
        "    data['workclass'] = le.transform(data['workclass'])\n",
        "    le.fit(data['education'])\n",
        "    data['education'] = le.transform(data['education'])\n",
        "    le.fit(data['marital-status'])\n",
        "    data['marital-status'] = le.transform(data['marital-status'])\n",
        "    le.fit(data['occupation'])\n",
        "    data['occupation'] = le.transform(data['occupation'])\n",
        "    le.fit(data['relationship'])\n",
        "    data['relationship'] = le.transform(data['relationship'])\n",
        "    le.fit(data['race'])\n",
        "    data['race'] = le.transform(data['race'])\n",
        "    le.fit(data['sex'])\n",
        "    data['sex'] = le.transform(data['sex'])\n",
        "    le.fit(data['native-country'])\n",
        "    data['native-country'] = le.transform(data['native-country'])\n",
        "    le.fit(data['id'])\n",
        "    data['id'] = le.transform(data['id'])\n",
        "    temp = data.drop(columns='id').copy()\n",
        "    data = data[temp.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]\n",
        "    return data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def one_hot_encoder(data):\n",
        "    # values = np.array(data)\n",
        "    values = data.to_numpy()\n",
        "    # integer encode\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded = label_encoder.fit_transform(values)\n",
        "    # binary encode\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "    return onehot_encoded\n",
        "\n",
        "\n",
        "def one_hot_encoder_without_sklean(data):\n",
        "    temp = data.drop(columns='id')\n",
        "    id = data.id\n",
        "    data = pd.get_dummies(temp, prefix_sep='_', drop_first=True)\n",
        "    data['id'] = id\n",
        "    data.head()\n",
        "    data = data.replace(['<=50K', '>50K'], [0, 1])\n",
        "    temp = data.drop(columns='id').copy()\n",
        "    data = data[temp.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]\n",
        "    return data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# show the distribution of the positive vs. negative classes\n",
        "def show_povsneg(data):\n",
        "    sumid = [(data.id == 0).sum(), (data.id == 1).sum()]\n",
        "    xl = ['<=50', '>50']\n",
        "    plt.bar(x=xl, height=sumid)\n",
        "    for i, v in zip(xl, sumid):\n",
        "        plt.annotate(str(v), xy=(i, v), color='black', va='center', size=11)\n",
        "    plt.ylabel('amount')\n",
        "    plt.xlabel('wage')\n",
        "    plt.title('distribution of different wages(adult)')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaJMrqOZD1El",
        "colab_type": "text"
      },
      "source": [
        "### Process Adult"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE1hwddD135R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_Adult():\n",
        "  data = pd.read_csv('adult.data', engine='python', sep=',\\s', na_values=['?'],\n",
        "                    names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
        "                           'marital-status', 'occupation','relationship', 'race', 'sex', \n",
        "                           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'id'])\n",
        "  target = data['id']\n",
        "\n",
        "  data = clean(data)\n",
        "  data = preprocess(data)\n",
        "\n",
        "  workclass = one_hot_encoder(data['workclass'])\n",
        "  # print(workclass.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(workclass)], axis=1)\n",
        "  education = one_hot_encoder(data['education'])\n",
        "  # print(education.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(education)], axis=1)\n",
        "  marital_status = one_hot_encoder(data['marital-status'])\n",
        "  # print(marital_status.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(marital_status)], axis=1)\n",
        "  occupation = one_hot_encoder(data['occupation'])\n",
        "  # print(occupation.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(occupation)], axis=1)\n",
        "  relationship = one_hot_encoder(data['relationship'])\n",
        "  # print(relationship.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(relationship)], axis=1)\n",
        "  race = one_hot_encoder(data['race'])\n",
        "  # print(race.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(race)], axis=1)\n",
        "  native_country = one_hot_encoder(data['native-country'])\n",
        "  # print(native_country.shape[1])\n",
        "  data = pd.concat([data, pd.DataFrame(native_country)], axis=1)\n",
        "\n",
        "  # data=data.replace(['Male','Female'],[1,0])\n",
        "  # data=data.replace(['<=50K','>50K'],[0,1])\n",
        "\n",
        "  del data['workclass']\n",
        "  del data['education']\n",
        "  del data['marital-status']\n",
        "  del data['occupation']\n",
        "  del data['race']\n",
        "  del data['relationship']\n",
        "  del data['native-country']\n",
        "  ids = data['id'].copy()\n",
        "  del data[\"id\"]\n",
        "  data.insert(data.shape[1], \"id\", ids)\n",
        "\n",
        "  adult_dataset_result = data.to_numpy()\n",
        "\n",
        "  return train_test_split(adult_dataset_result, 9, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYvBPhE2HZPR",
        "colab_type": "text"
      },
      "source": [
        "### Process Adult Raw (No One-Hot Encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PTiA1r6HQDr",
        "colab_type": "code",
        "outputId": "496b5f4d-9466-4a11-c228-57bfbd8c89ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        }
      },
      "source": [
        "def process_Adult_Raw():\n",
        "  adult_dataframe = pd.read_csv(\"preprocess_adult.csv\",header=None)\n",
        "  #Binarize classifications\n",
        "  classIndex = len(adult_dataframe.keys()) -1\n",
        "  for i in range(len(adult_dataframe)):\n",
        "    if adult_dataframe[classIndex][i] == \" >50K\":\n",
        "      adult_dataframe[classIndex][i] = 1\n",
        "    else:\n",
        "      adult_dataframe[classIndex][i] = 0\n",
        "\n",
        "  return train_test_split(adult_dataframe, 9, False)\n",
        "process_Adult_Raw()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3441",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-14dabac972a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madult_dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprocess_Adult_Raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-132-14dabac972a4>\u001b[0m in \u001b[0;36mprocess_Adult_Raw\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0madult_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madult_dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprocess_Adult_Raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-def50b049954>\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(mydataset, k, Normalize)\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mmydataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmydataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmydataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmydataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.shuffle\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.shuffle\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3441"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrTXe9fNEVZ3",
        "colab_type": "text"
      },
      "source": [
        "### Process wines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znUi6m7Z_KI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_wines():\n",
        "    with open(\"winequality-white.csv\", 'r') as f:\n",
        "        wines = list(csv.reader(f, delimiter=\";\"))\n",
        "    global wines_header\n",
        "    wines_header = np.array(wines[0])  # with label header\n",
        "    wines = np.array(wines[1:], dtype=np.float)  # with label\n",
        "\n",
        "    # clean malinformed values by deleting the rows they inhabit\n",
        "    invalid_index = []\n",
        "    for i in range(len(wines)):\n",
        "        for number in wines[i]:\n",
        "            if math.isnan(number):\n",
        "                np.delete(wines, i, 0)\n",
        "\n",
        "    # differentiate labels\n",
        "    for i in tqdm(range(len(wines[:, -1]))):\n",
        "        if wines[:, -1][i] > 5:\n",
        "            wines[:, -1][i] = 1\n",
        "        else:\n",
        "            wines[:, -1][i] = 0\n",
        "\n",
        "    return train_test_split(wines, 9, True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zweg_4jMroS",
        "colab_type": "text"
      },
      "source": [
        "### Process Breast Cancer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr79YJ7IAibJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_tumors():\n",
        "    with open(\"breast-cancer-wisconsin.csv\", 'r') as f:\n",
        "        tumors = list(csv.reader(f, delimiter=\";\"))\n",
        "\n",
        "    global tumors_header\n",
        "    tumors_header = [\"clump thickness\", \"cell size\", \"cell shape\", \"marginal adhesion\", \\\n",
        "                     \"single epithelial cell size\", \"number of bare nuclei\", \"bland chromatin\", \\\n",
        "                     \"number of normal nuclei\", \"mitosis\", \"label\"]  # with label header but no IDs\n",
        "\n",
        "    # highlight malinformed values\n",
        "    invalid_index = []\n",
        "    for i in tqdm(range(len(tumors))):\n",
        "        tumors[i] = tumors[i][0].split(\",\")\n",
        "        for j in range(len(tumors[i])):\n",
        "            if tumors[i][j].isnumeric() == False:\n",
        "                invalid_index.append(i)  # the whole row\n",
        "        # differentiate labels\n",
        "        if int(tumors[i][-1]) <= 2:\n",
        "            tumors[i][-1] = '0'\n",
        "        else:\n",
        "            tumors[i][-1] = '1'\n",
        "\n",
        "    # clean malinformed values by deleting the rows they inhabit\n",
        "    invalid_index.sort(reverse=True)\n",
        "    for i in invalid_index:\n",
        "        tumors.remove(tumors[i])\n",
        "\n",
        "    tumors = np.array(tumors[0:], dtype=np.float)\n",
        "    tumors = tumors[:, 1:]\n",
        "\n",
        "    return train_test_split(tumors, 9, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZctIbJ0oIJUh",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Um8SDt_5SE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation(prediction: np.ndarray, groundtruth: np.ndarray):\n",
        "    # sanity check\n",
        "    if len(prediction) != len(groundtruth):\n",
        "        raise TypeError\n",
        "    \n",
        "    tn,fp,fn,tp = 0,0,0,0 #true negative, false positive, false negative, true positive\n",
        "    \n",
        "    for i in range(len(prediction)):\n",
        "        if prediction[i] == 0 and groundtruth[i] == 0:\n",
        "            tn += 1\n",
        "        if prediction[i] == 1 and groundtruth[i] == 0:\n",
        "            fp += 1\n",
        "        if prediction[i] == 0 and groundtruth[i] == 1:\n",
        "            fn += 1\n",
        "        if prediction[i] == 1 and groundtruth[i] == 1:\n",
        "            tp += 1\n",
        "    return tn,fp,fn,tp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBchU2P__rNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_chunks(data_split,indices):\n",
        "    indices = list(indices).sort()\n",
        "    if len([indices]) < 2:\n",
        "        return data_split[0]\n",
        "    data_merged = data_split[indices[0]]\n",
        "    indices.remove(indices[0]) #remove the first element so that it does not get re-merged\n",
        "    for i in indices:\n",
        "        data_merged = np.concatenate(data_merged,data_split[i],axis=0)\n",
        "        \n",
        "    return data_merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnfVTjMmDVBn",
        "colab_type": "text"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgyvHBU4-uls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "#============================================================================================================================================================================================================================================================================\n",
        "#\n",
        "# K-CROSS VALIDATION\n",
        "#\n",
        "#   @Author: Edwin Pan of Group 101 of Winter 2020 COMP551 at McGill University\n",
        "#\n",
        "#   This section is a script for applying K-Cross Validation \n",
        "#\n",
        "#============================================================================================================================================================================================================================================================================\n",
        "\n",
        "def kCrossValidate(k,cleanModel,featuresDataset,classificationsDataset,learningRate=None,gradientDescentIterations=None):\n",
        "\n",
        "  #Create a list ready to keep track of all the models ever made.\n",
        "  allKModels = []\n",
        "\n",
        "  #First, create all of the dataset partitions needed. \n",
        "  features_test_block =             []\n",
        "  classifications_test_block  =     []\n",
        "  features_training_block =         []\n",
        "  classifications_training_block =  []\n",
        "  dataset_size = classificationsDataset.size  #Total number of instances\n",
        "  for fold in range(k):\n",
        "    #Get the subsets\n",
        "    lower_index = dataset_size*fold//k        #Get the lower bound index from the full dataset for the test dataset\n",
        "    upper_index = dataset_size*(fold+1)//k    #Get the upper bound idnex from the full dataset for the test dataset\n",
        "    #The Test Block of size 1/k per k\n",
        "    features_test_block.append(         featuresDataset[(lower_index):(upper_index)]                )\n",
        "    classifications_test_block.append(      classificationsDataset[(lower_index):(upper_index)]                 )\n",
        "    #The Training Block of size (k-1)k per k\n",
        "    if(lower_index==0):   #If we're dealing with the first fold\n",
        "      features_training_block.append(                featuresDataset[upper_index:] )\n",
        "      classifications_training_block.append(  classificationsDataset[upper_index:]   ) \n",
        "    elif(upper_index == dataset_size):    #If we're dealing with the last fold\n",
        "      features_training_block.append(                featuresDataset[:lower_index] )  \n",
        "      classifications_training_block.append(  classificationsDataset[:lower_index]  )\n",
        "    else:\n",
        "      #Note that these two following operations only work if lower_index is not 0 and upper_index is not the upper bound such that we get empty matrices.\n",
        "      new_block_size = dataset_size - upper_index + lower_index;\n",
        "      no_of_features = len(featuresDataset[0])\n",
        "      #Before making the block, we need to make sure our new array will contain the right datatype. If we find string, make our array hold objects.\n",
        "      datatype = type(float)\n",
        "      for f in range(no_of_features):\n",
        "        if not isinstance( featuresDataset[0][f], float ) and not isinstance( featuresDataset[0][f], int):\n",
        "          datatype = object\n",
        "          break\n",
        "      #Make the new block\n",
        "      new_features_block = np.zeros( (new_block_size,no_of_features), dtype=datatype )\n",
        "      new_classes_block = np.zeros( new_block_size )\n",
        "      j = 0 #Row being checked\n",
        "      for i in range( lower_index ):\n",
        "        for f in range(no_of_features):\n",
        "          new_features_block[j][f] = featuresDataset[i][f]\n",
        "        new_classes_block[j] = classificationsDataset[i]\n",
        "        j += 1\n",
        "      for i in range( upper_index, dataset_size ):\n",
        "        for f in range(no_of_features):\n",
        "          new_features_block[j][f] = featuresDataset[i][f]\n",
        "        new_classes_block[j] = classificationsDataset[i]\n",
        "        j += 1\n",
        "      features_training_block.append( new_features_block )\n",
        "      classifications_training_block.append( new_classes_block )\n",
        "      #features_training_block.append(         np.concatenate(featuresDataset[:lower_index],(featuresDataset[upper_index:])) )   \n",
        "      #classifications_training_block.append(  np.concatenate(classificationsDataset[:lower_index],(classificationsDataset[upper_index:]))   ) \n",
        "      \n",
        "  #Now we start training and testing the model.\n",
        "  average_accuracy = 0;\n",
        "  for fold in range(k):\n",
        "    model = copy.deepcopy(cleanModel)\n",
        "    if(learningRate==None or gradientDescentIterations==None):\n",
        "      model.fit(trainingDataFeatures= features_training_block[fold], \n",
        "                trainingDataClasses=  classifications_training_block[fold]\n",
        "                )\n",
        "    else:\n",
        "      model.fit(trainingDataFeatures=     features_training_block[fold], \n",
        "                trainingDataClasses=      classifications_training_block[fold], \n",
        "                learningRate=             learningRate, \n",
        "                gradientDescentIterations=gradientDescentIterations,\n",
        "                beVerbose= False\n",
        "                )\n",
        "    fold_accuracy = model.evaluate_acc( X=(features_test_block[fold]), Y=(classifications_test_block[fold]) )\n",
        "    average_accuracy += fold_accuracy\n",
        "    #print(\"Accuracy of fold number \" + str(fold+1) + \" is \" + str(fold_accuracy) + \".\")\n",
        "    allKModels.append(model)\n",
        "    continue\n",
        "  average_accuracy /= k\n",
        "  #Return the average accuracy obtained\n",
        "  return average_accuracy, allKModels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVyEaMzXzt-4",
        "colab_type": "text"
      },
      "source": [
        "## Model Averager\n",
        "\n",
        "Given enough time, we would like to combine all of the k models obtained through the k-fold cross validation to create a new model for use on predicting the classifications on the test dataset. Combining of the models would come down to essentially averaging each weight among the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptcz-tLhz0u0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "def averageHNBModels(models):\n",
        "  numberOfModels = len(models)\n",
        "  newModel = copy.deepcopy(model[0])\n",
        "  for k, v in newModel.feature_models.values():\n",
        "    if newModel.feature_models[k].__type__ == \"BINARY\":\n",
        "      average_featuremodel_mean = 0\n",
        "      average_featuremodel_variance = 0\n",
        "      for m in models:\n",
        "        m.feature_models[k].mean\n",
        "    elif newModel.feature_models[k].__type__ == \"CATEGORICAL\":\n",
        "      pass\n",
        "    elif newModel.feature_models[k].__type__ == \"GAUSSIAN\":\n",
        "      pass\n",
        "      #INCOMPLETE CODE\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7xe_cDnDY22",
        "colab_type": "text"
      },
      "source": [
        "## Four Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otg7t5fdlA89",
        "colab_type": "code",
        "outputId": "9178b74c-1511-403b-f2ec-14b68e2ac56b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import time\n",
        "ionosphere_train_x, ionosphere_train_y, ionosphere_test_x, ionosphere_test_y = process_ionosphere()\n",
        "start = time.time()\n",
        "ionosphere_HNBModel = HybridNaiveBayesModel( len(ionosphere_train_x[0]), 2 )\n",
        "average_accuracy, allModels = kCrossValidate(5,ionosphere_HNBModel,ionosphere_train_x,ionosphere_train_y)\n",
        "ionosphere_HNBModel.fit(ionosphere_train_x,ionosphere_train_y)\n",
        "final_test_accuracy = ionosphere_HNBModel.evaluate_acc(ionosphere_test_x,ionosphere_test_y)\n",
        "end = time.time()\n",
        "print(\"Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is \" + str(average_accuracy) +\".\")\n",
        "print(\"Final accuracy on Ionosphere's testing set: \", final_test_accuracy)\n",
        "print(\"Hybrid Naive Bayes on Ionosphere train set time: \", (end - start)/5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is 0.8222222222222223.\n",
            "Final accuracy on Ionosphere's testing set:  0.7777777777777778\n",
            "Hybrid Naive Bayes on Ionosphere train set time:  0.060173559188842776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bby3b1UR_3iY",
        "colab_type": "code",
        "outputId": "3fd6edca-8846-4854-c5f9-1332dba75b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import time\n",
        "cancer_train_x, cancer_train_y, cancer_test_x, cancer_test_y = process_tumors()\n",
        "start = time.time()\n",
        "cancer_HNBModel = HybridNaiveBayesModel( len(cancer_train_x[0]), 2 )\n",
        "average_accuracy, allModels = kCrossValidate(5,cancer_HNBModel,cancer_train_x,cancer_train_y)\n",
        "cancer_HNBModel.fit(cancer_train_x,cancer_train_y)\n",
        "final_test_accuracy = cancer_HNBModel.evaluate_acc(cancer_test_x,cancer_test_y)\n",
        "end = time.time()\n",
        "print(\"Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is \" + str(average_accuracy) +\".\")\n",
        "print(\"Final accuracy on Cancer's testing set: \", final_test_accuracy)\n",
        "print(\"Hybrid Naive Bayes on Cancer train set time: \", (end - start)/5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 699/699 [00:00<00:00, 166808.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is 0.965813674530188.\n",
            "Final accuracy on Cancer's testing set:  0.9420289855072463\n",
            "Hybrid Naive Bayes on Cancer train set time:  0.07291746139526367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLQmSwFwsplA",
        "colab_type": "code",
        "outputId": "7965aaf5-68c3-4edc-f942-7d0bade1d29b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import time\n",
        "Adult_train_x, Adult_train_y, Adult_test_x, Adult_test_y  = process_Adult()\n",
        "start = time.time()\n",
        "adult_HNBModel = HybridNaiveBayesModel( len(Adult_train_x[0]), 2 )\n",
        "average_accuracy, allModels = kCrossValidate(5,adult_HNBModel,Adult_train_x,Adult_train_y)\n",
        "adult_HNBModel.fit(Adult_train_x,Adult_train_y)\n",
        "final_test_accuracy = adult_HNBModel.evaluate_acc(Adult_test_x,Adult_test_y)\n",
        "end = time.time()\n",
        "print(\"Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is \" + str(average_accuracy) +\".\")\n",
        "print(\"Final accuracy on Adult's testing set: \", final_test_accuracy)\n",
        "print(\"Hybrid Naive Bayes on Adult train set time: \", (end - start)/5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:250: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is 0.23347993583516163.\n",
            "Final accuracy on Adult's testing set:  0.23408385093167702\n",
            "Hybrid Naive Bayes on Adult train set time:  13.64684796333313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhglqTPR-0Ho",
        "colab_type": "code",
        "outputId": "f5244c32-b294-469d-8b24-9d77943a3568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import time\n",
        "x_wines_train, y_wines_train, x_wines_test, y_wines_test = process_wines()\n",
        "start = time.time()\n",
        "wine_HNBModel = HybridNaiveBayesModel( len(x_wines_train[0]), 2 )\n",
        "average_accuracy, allModels = kCrossValidate(5,wine_HNBModel,x_wines_train,y_wines_train)\n",
        "wine_HNBModel.fit(x_wines_train,y_wines_train)\n",
        "final_test_accuracy = wine_HNBModel.evaluate_acc(x_wines_test,y_wines_test)\n",
        "end = time.time()\n",
        "print(\"Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is \" + str(average_accuracy) +\".\")\n",
        "print(\"Final accuracy on Wines testing set: \", final_test_accuracy)\n",
        "print(\"Hybrid Naive Bayes on Wines train set time: \", (end - start)/5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4898/4898 [00:00<00:00, 482099.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hybrid Naive Bayes K-Cross Validated (K=5) accuracy of our model is 0.7009942834492858.\n",
            "Final accuracy on Wines testing set:  0.746938775510204\n",
            "Hybrid Naive Bayes on Wines train set time:  0.5992419719696045\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}